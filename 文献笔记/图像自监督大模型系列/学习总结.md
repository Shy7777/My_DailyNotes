# MOCO
## 1.什么是对比学习。
### 原理 ：对比学习是无监督学习的一种，着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。 
- 举个例子，从imagenet中抽出猫、猫、狗、飞机四张图，那么猫和猫的图片肯定是相似的，和狗不相似。但是和飞机比起来，猫和狗是相似的。所以对比学习就是对比着差异去学习，模型并不需要真的知道图片中代表的是什么，而只需要知道哪些图片是类似的，哪些图片是不一样的就可以了。
### 训练目的：
- 对比学习，希望相似数据（图片）最终学到的特征是相似的，在特征空间（embedding space ）中，特征向量尽量靠近；反之还希望不同的数据学到的特征向量，尽量远离。
### pretext task（代理任务）：
- 对比学习是不需要标签的（比如不需要知道图片是哪一类），但模型还是需要知道哪些图片是类似的，哪些是不相似的，才能训练。这就需要通过通过设计一些巧妙的代理任务，人为指定一些任务来实现。
### 应用最广的代理任务：instance discrimination 。
## 2.学习了对比学习框架，现阶段对比学习有三种框架：
- 一种是端到端的，比如SimCLR
- 利用memory bank的
- 最后一种就是MOCO
## 3.解决了什么
- 这种对比学习既解决了batch size无法过大的问题，利用队列算法，然而SimCLR利用其硬件优势强行把batch size拉大
- 也解决了key编码器不一致的问题：
  - 常见的key编码器比如端到端，是会随着batch来进行实时更新，很容易导致不一致
  - 而memory bank的对比学习框架，利用离线的bank来存储数据集，但由于这些特征是不同时刻的编码器得到的，所以一致性问题
  - MOCO利用动量解决了这个问题

# MOCO
这篇论文是由何凯明及其团队提出的，发表在期刊上，对于提出的将对比学习作为字典查找我认为是很有意义的一个想法。
### 研究背景：
- 目前无监督学习在自然语言处理方面发挥出了很显著的作用如GPT，但是在CV方面还是难以发挥，还是由于作为自然语言，是一个离散形式的数据特征，维度低便于构建标记化字典，但是图像作为高维度空间且具有连续的数据特征，难以构建。
- 而当时CV对于无监督训练还是有一些进展，就是通过对比学习，故团队提出将对比学习抽象作字典一样的数据结构，图像编码器输入的图像就是一个键，来查找相似的值，这就是对比学习了
- 而从结果上也确实有很大的成功，在7个下游任务中均超过了监督学习的分数
### 训练方法：
- 思路就是存在两个编码器，一个encoder，一个momentum encoder，其中精髓在于采用队列输入，大大提高了batch_size的大小，并且将另一个编码器进行动量更新，保证一致性同时也加入训练
- 损失函数使用的是InfoNCE
- 文中还对比了三种对比学习的架构

# SimCLR
# SwAV
最大的区别主要在于前者其实更多focus在对比任务的设计上，即怎么去对比的问题，而不在 已知怎么对比的情况下，去上各种各样的优化手段增强对比的效果