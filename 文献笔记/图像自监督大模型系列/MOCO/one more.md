
---
Key encoder（键编码器）是对比学习框架中用于生成“键”向量（key representations）的神经网络模块。它的作用是将输入样本（通常是图像的某种视图）编码成一个向量，用于与查询向量（query）进行相似度比较，从而计算对比损失。

---

## 在 MoCo 中，key encoder 的定义与作用

在 MoCo 框架中，存在两个编码器：

| 编码器 | 输入 | 输出 | 作用 |
|--------|------|------|------|
| **Query Encoder** \( f_q \) | 图像视图 \( x_q \) | 查询向量 \( q \) | 用于构造对比损失的“查询” |
| **Key Encoder** \( f_k \) | 图像视图 \( x_k \) | 键向量 \( k \) | 用于构造对比损失的“键” |

MoCo 的训练目标是让查询向量 \( q \) 与其对应的正样本键 \( k^+ \) 相似，与其他负样本键 \( k^- \) 不相似。

---

## 为什么 key encoder 要用动量更新？

MoCo 的关键创新之一是使用动量更新的 key encoder，而不是直接复制 query encoder。这是因为：

- 队列中的键向量来自多个 mini-batch，不能回传梯度
- 如果直接复制 query encoder，编码器变化太快，导致键向量不一致
- 动量更新可以让 key encoder 平滑演化，保持一致性

动量更新公式如下：

\[
\theta_k \leftarrow m \cdot \theta_k + (1 - m) \cdot \theta_q
\]

其中：

- \( \theta_k \)：key encoder 的参数
- \( \theta_q \)：query encoder 的参数
- \( m \)：动量系数，通常设为 0.999

---

## 实验验证：动量对 key encoder 的影响

论文中做了动量系数的消融实验，结果如下：

| 动量 \( m \) | 准确率 (%) |
|-------------|------------|
| 0           | 训练失败   |
| 0.9         | 55.2       |
| 0.99        | 57.8       |
| 0.999       | 59.0       |
| 0.9999      | 58.9       |

说明：key encoder 的一致性对训练稳定性和性能至关重要。

---

## 总结

**Key encoder 是 MoCo 中用于生成键向量的编码器，其核心作用是构建对比学习中的负样本池。通过动量更新，它保持编码器的一致性，使得训练更加稳定和高效。**

---
## 对比学习灵活性在于：
    可以灵活定义正样本和负样本的规则
## 有监督学习和无监督学习： 
    无监督学习或自监督学习就是没有标签，通过代理任务或人为定义的规则来生成标签
### 当Q和正样本相似，和负样本不相似，loss低
### 当Q和正样本不相似，和负样本相似，loss高
# METHOD
## 使用的损失函数：InfoNCE（noise contrastive estimation）
- 由于对比学习会有大量的分类类别，这导致softmax无法工作，这时就无法使用交叉熵损失函数
- 而文中引入NCE，使得一类作为data sample，一类作为noise sample
- 但noise sample仍然很大，所以抽取其中一些作为sample
- 温度超参数的选择，t越大越平滑，对比损失对于所有的负样本都一视同仁，过小会使得过于关注特别困难的样本
## 在更新时，每一个miniBatch都会有新的key进去旧的key出去，根据FIFO，如果要训练一个好的队列，那一个缓慢编码器是有必要的
# 三种训练方式
## 端到端训练
- SimCLR本质上就是端到端的训练模式，但由于谷歌硬件足够强，所以batchsize可以很大
- 好处就是编码器可以随时更新，所以key的一致性非常好
## memory bank
- 牺牲一致性，着重于batchsize，离线操作，把所有数据集存入bank中
- 具体训练：先随机在bank中取出一些sample，随后和q进行损失计算，而后更新q的编码器，再更新key放回bank中去
- 由于这些特征是不同时刻的编码器得到的，所以一致性问题
## MOCO
    