## Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints  重新思考数据约束下原生多模态大语言模型的规模特性

简要：主要是探究，在当前多模态预训练的范式下，多模态的缩放特性是怎么样的规律

背景：组合训练的范式，但不明确的缩放特性

当前范式：大多数现有的 MLLM 采用了组合范式，该范式通过投影器连接预训练的视觉编码器 VLP:CLIP 和 LLM qwen，并在对齐上进行微调。然后，整个结构将在多模态数据上进行进一步微调以实现对齐。基于这一范式，现有工作主要集中于视觉编码器的改进（Qwen2vl；wang2023internimage；llava-hr）和连接器的设计（li2022blip；VLM:LLaVA）。尽管取得了进展，但这种范式仍难以探索视觉和语言的联合规模特性。社区也逐渐认识到它们在训练流程和视觉语言对齐方面的潜在局限性。

抛出两大问题：
	1.如何对于原生架构关键组件的选择
	2.基于原生架构的最佳选择，探索他们的缩放特性

探索架构组件的最优设计：
	1.LLM初始化
		结论：从预训练的LLM初始化极大地促进了多模态数据的收敛，在大多数情况下，即使多模态数据量大，也能提供更好的性能。
	2.MoE的有效性：
		比较，一种包含视觉编码器和通用LLM，另一组包含视觉编码器和MoE架构的LLM
		结论：MoE显著提升模型性能，同时不增加激活参数数量。
	3.视觉编码器的最优架构
		结论：视觉编码器在广泛的深度和宽度配置下都能实现近乎最佳的性能。浅编码器在早期训练中收敛速度更快，而深度编码器在更大数据集时表现略佳。

拓展原生MLLMs：
	基于架构组建的最优解，探究
	1）独立扩展视觉编码器和 LLM 的影响；
		结论：扩展LLM能够持续提升多模态性能，遵循典型的LLM扩展规律。然而，增加视觉编码器体积后收益递减，表明MLLM的性能受限于LLM的容量。
	2）同时扩展视觉编码器和 LLM 的最佳方式。
		结论：视觉编码器的最佳尺寸与LLM的对数尺度成比例缩放，表明两个组件应联合进行缩放。这进一步表明，使用单一预训练视觉编码器覆盖多种大型语言模型尺度（如现有合成MLLM）的预训练视觉编码器是次优的。
	


Q：什么是缩放特性
A：
1. 大语言模型的缩放特性

	指模型**性能（如损失降低、任务准确率提升）与关键资源（参数规模、训练数据量、计算量）之间的可预测规律**，核心是 “资源增长如何系统性带动性能提升”—— 比如典型的 “对数线性缩放律”：参数 / 数据量指数级增加时，模型损失会近似线性下降，且这种关系可提前预判。

2. 存在缩放特性不明确

	结合论文语境，指组合式 MLLMs（分离预训练视觉编码器与 LLM 再融合）的训练模式，导致**视觉与语言组件的 “联合缩放规律” 无法确定**：比如增大 LLM 参数后，视觉编码器该同步扩大多少才能最优？两者缩放是否有协同效应？这些问题缺乏明确答案，无法像纯 LLM 那样预判缩放后的性能收益。

作者提出了问题：
	有限的数据和大规模训练带来了巨大挑战。因此，一个关键的实际问题仍然存在：原生MLLMs能否以及如何以可接受的成本切实达到甚至超越顶级MLLMs的性能上限。

Q：什么是原生的MLLMs
A：
1. 原生 MLLMs（原生多模态大语言模型）核心定义
	指以端到端方式联合优化视觉与语言能力**的模型，不分离预训练视觉编码器和 LLM，而是用统一训练目标（如下一个 token 预测）让两者从训练初期就协同学习，最大化视觉 - 语言对齐，无需额外连接器（如组合式模型的 projector）。
2. 关键特征（对比组合式 MLLMs）
	- 训练范式：全程端到端，视觉、语言组件共享同一训练目标，而非 “先分别预训、再融合对齐”；
	- 组件关系：视觉编码器与 LLM 需**联合缩放**（最优视觉编码器大小随 LLM 规模对数比例增长），而非固定某一尺寸；
	- 架构设计：常集成 MoE（混合专家）架构，适配异质的视觉 / 语言数据，且多基于预训练 LLM 初始化以提升收敛效率。


